require('onmt.init')
require('tds')

local translator_ab = nil
local translator_ba = nil

local trainer_ab = nil
local trainer_ba = nil

local validData_ab = nil
local validData_ba = nil

-- backup value for N
local n_best_backup = 1
-- first batch, used for memory optimization
local batch_first_for_optim = nil

local cmd = onmt.utils.ExtendedCmdLine.new('dual.lua')

-- First argument define the model type: seq2seq/lm - default is seq2seq.
local modelType = cmd.getArgument(arg, '-model_type') or 'seq2seq'
local modelClass = onmt.ModelSelector(modelType)

local options = {
  {
    '-dual', false,
    [[Must set to distinguish between normal training.]]
  },
  {
    '-train_src', '',
    [[Sentence sequences to train dual systems.
    Build from pre-processing-dual.]],
    {
      valid = onmt.utils.ExtendedCmdLine.fileExists
    }
  },
  {
    '-model_ba', '',
    [[Path to the serialized model B-->A file.]],
    {
      valid = onmt.utils.ExtendedCmdLine.fileExists
    }
  },
  {
    '-data', '',
    [[Path to the data package `*-train.t7` generated by the preprocessing step.]],
    {
      valid = onmt.utils.ExtendedCmdLine.fileExists
    }
  },
  {
    '-data_ba', '',
    [[Path to the data package `*-train.t7` for B-->A generated by the preprocessing step.]],
    {
      valid = onmt.utils.ExtendedCmdLine.fileExists
    }
  },
  {
    '-report_ppl_every_N_batches', 50,
    [[Optional, evaluate ppl after every N batches]],
    {
      valid = onmt.utils.ExtendedCmdLine.isInt(1)
    }
  }
}

cmd:setCmdLineOptions(options, 'Data')

onmt.translate.Translator.declareOpts(cmd)
onmt.utils.Cuda.declareOpts(cmd)
onmt.train.Trainer.declareOpts(cmd)
modelClass.declareOpts(cmd)
onmt.utils.Logger.declareOpts(cmd)
onmt.utils.Memory.declareOpts(cmd)

local function loadDataset(filename)
  _G.logger:info('Loading data from \'%s\'...', filename)

  local dataset = torch.load(filename, 'binary', false)

  -- Keep backward compatibility.
  dataset.dataType = dataset.dataType or 'bitext'

  -- Check if data type is compatible with the target model.
  if modelClass.dataType() ~= dataset.dataType then
    _G.logger:error('Data type `%s\' is incompatible with `%s\' models',
                    dataset.dataType, modelClass.modelName())
    os.exit(0)
  end

  return dataset
end

local function decode_n_best_each(opt, srcBatch, batchId, output, flag_is_first, translator)
  local output_each = {}

  if flag_is_first == true then
    for b = 1, #srcBatch do
      srcBatch[b] = translator:buildInput(srcBatch[b])
    end
    opt.n_best = n_best_backup  -- set A->B to N-best
  else
    for b = 1, #output do 
      table.insert(srcBatch, translator:buildInput(output[b][3]))
    end
    opt.n_best = 1  -- set B->A to 1-best
  end

  local results = translator:translate(srcBatch, nil)

  for b = 1, #results do
    if (srcBatch[b].words and #srcBatch[b].words == 0) then
      _G.logger:warning('Line ' .. batchId .. ' is empty.')
    else
      for n = 1, #results[b].preds do
        local sentence = results[b].preds[n].words
        -- BA_translation, BA_trans_score, AB_translation_{k}, source_sentence
        if flag_is_first == true then
          table.insert(output_each, {nil, results[b].preds[n].score, sentence, srcBatch[b]})
        else
          -- -- BLEU as reward
          -- -- Use Formular from https://hal.archives-ouvertes.fr/hal-00331752v1/document
          local refs = {}
          table.insert(refs, {output[b][4]["words"]})          
          local bleu_score = onmt.scorers.bleu({sentence}, refs)
          local beta = 0.5
          local delta = 0.5
          local reward = beta * math.exp(-1 * bleu_score * bleu_score / 2 / delta / delta)
          
          -- -- log P(s|s_mid) as reward
          -- local reward = results[b].preds[n].score;

          table.insert(output_each, {sentence, reward, srcBatch[b], output[b][4]})
        end
      end
    end -- if
  end

  srcBatch = {}
  results = {}
  collectgarbage()
  
  return output_each
end

local function decode_n_best_ab(opt, srcBatch, batchId, output, flag_is_first)
  cutorch.setDevice(1)
  if translator_ab == nil then
    translator_ab = onmt.translate.Translator.new(opt)
  end
  translator_ab.model:evaluate()
  
  return decode_n_best_each(opt, srcBatch, batchId, output, flag_is_first, translator_ab)
end
  
local function decode_n_best_ba(opt, srcBatch, batchId, output, flag_is_first)
  cutorch.setDevice(2)
  if translator_ba == nil then
    opt.model = opt.model_ba
    translator_ba = onmt.translate.Translator.new(opt)
  end
  translator_ba.model:evaluate()

  return decode_n_best_each(opt, srcBatch, batchId, output, flag_is_first, translator_ba)
end
  
local function buildData_batch(opt, output, dicts, flag_src_tgt)
  local index = 1
  local index_k_batch = 1
  local k_batch = {}
  local k_batch_r2 = {}
  
  for index_k_batch = 1, n_best_backup do
    local src = {}
    local tgt = {}
    local r2 = 0

    for index = index_k_batch, #output, n_best_backup do
      local src_sent = {}
      if flag_src_tgt == true then
        src_sent = output[index][4].words
      else
        src_sent = output[index][3].words
      end
      local t = torch.Tensor(#src_sent)
      for i = 1, #src_sent do
        t[i] = dicts.src.words:lookup(src_sent[i])
      end
      table.insert(src, t)
      
      local tgt_sent = {}
      if flag_src_tgt == true then
        tgt_sent = output[index][3].words
      else
        tgt_sent = output[index][4].words
      end
      -- add <s> at the beginning and </s> at the end
      local t = torch.Tensor(#tgt_sent+2)
      t[1] = 3  -- <s>
      for i = 2, #tgt_sent+1 do
        t[i] = dicts.tgt.words:lookup(tgt_sent[i-1])
      end
      t[#tgt_sent+2] = 4  -- </s>
      
      table.insert(tgt, t)
      
      -- normalize by sentence length
      r2 = r2 + output[index][2]/(#src_sent)
    end
    
    table.insert(k_batch, onmt.utils.Cuda.convert(onmt.data.Batch.new(src, nil, tgt, nil)))
    table.insert(k_batch_r2, r2)
  end
  
  return k_batch, k_batch_r2
end

local function build_trainer(opt, model, dicts)
  model:training()
  onmt.utils.Cuda.convert(model)

  local trainer = onmt.train.Trainer.new(opt, model, dicts, batch_first_for_optim[1])
  return trainer
end

local function train_ab(opt, output, flag_is_first)
  ----------------------------------- AB --------------------------------------------
  -- build directly from training source (which has been randomized by pre-processing)
  
  local model = {}
  
  cutorch.setDevice(1)
  model = translator_ab.model
  local dicts = translator_ab.dicts
  
  if batch_first_for_optim == nil then
    batch_first_for_optim = buildData_batch(opt, output, dicts, true)
  end
  
  if trainer_ab == nil then
    trainer_ab = build_trainer(opt, model, dicts)
  else
    model:training()
  end
  
  if flag_is_first == true then
    local trainData_batch, batch_r = buildData_batch(opt, output, dicts, true)
    trainer_ab:train_dual_first(trainData_batch, batch_r, n_best_backup)
  else
    local trainData_batch = buildData_batch(opt, output, dicts, false)
    trainer_ab:train_dual_second(trainData_batch, n_best_backup)
  end
  
  translator_ab.model = trainer_ab.model
  collectgarbage() 
end

local function train_ba(opt, output, flag_is_first)
  ----------------------------------- BA --------------------------------------------
  -- build directly from training source (which has been randomized by pre-processing)
  
  local model = {}
  
  cutorch.setDevice(2)
  model = translator_ba.model
  local dicts = translator_ba.dicts

  if batch_first_for_optim == nil then
    batch_first_for_optim = buildData_batch(opt, output, dicts, false)
  end
  
  if trainer_ba == nil then
    trainer_ba = build_trainer(opt, model, dicts)
  else
    model:training()
  end
  
  if flag_is_first == true then
    local trainData_batch, batch_r = buildData_batch(opt, output, dicts, true)
    trainer_ba:train_dual_first(trainData_batch, batch_r, n_best_backup)
  else
    local trainData_batch = buildData_batch(opt, output, dicts, false)
    trainer_ba:train_dual_second(trainData_batch, n_best_backup)
  end

  translator_ba.model = trainer_ba.model
  collectgarbage() 
end

local function eval_and_save_ab(opt, flag_save)
  if validData_ab == nil then
    local dataset = loadDataset(opt.data)
    validData_ab = onmt.data.Dataset.new(dataset.valid.src, dataset.valid.tgt)
    validData_ab:setBatchSize(opt.max_batch_size, opt.uneven_batches)
  end
  
  cutorch.setDevice(1)

  trainer_ab:eval_and_save(opt, validData_ab, ".ab", flag_save)
  
  collectgarbage() 
end

local function eval_and_save_ba(opt, flag_save)
  if validData_ba == nil then
    local dataset = loadDataset(opt.data_ba)
    validData_ba = onmt.data.Dataset.new(dataset.valid.src, dataset.valid.tgt)
    validData_ba:setBatchSize(opt.max_batch_size, opt.uneven_batches)
  end
  
  cutorch.setDevice(2)

  trainer_ba:eval_and_save(opt, validData_ba, ".ba", flag_save)
  
  collectgarbage() 
end

local function main()
  local opt = cmd:parse(arg)

  _G.logger = onmt.utils.Logger.new(opt.log_file, opt.disable_logs, opt.log_level)
  _G.profiler = onmt.utils.Profiler.new()

  onmt.utils.Cuda.init(opt)

  local srcReader = onmt.utils.FileReader.new(opt.train_src, opt.idx_files, false) -- 3rd param: translator:srcFeat() is set to false at this time
  local srcBatch = {}

  local batchId = 1
  local flag_train_start_from_A = true
  
  n_best_backup = opt.n_best  -- keep N
  
  while true do
    local srcSeq = srcReader:next()
    
    if srcSeq ~= nil then
      table.insert(srcBatch, srcSeq)
    elseif #srcBatch == 0 then
      break
    end

    if srcSeq == nil or #srcBatch == opt.max_batch_size then
      if flag_train_start_from_A == true then
        -- sample sentences from language A
        _G.logger:info('BATCH %d: %s', batchId, "start to decode A-->B N-best")
        local output_ab = decode_n_best_ab(opt, srcBatch, batchId, nil, true)
        
        _G.logger:info('BATCH %d: %s', batchId, "start to decode B-->A 1-best")
        srcBatch = {} -- build srcBatch from previous output
        local output_ba = decode_n_best_ba(opt, srcBatch, batchId, output_ab, false)
        
        _G.logger:info('BATCH %d: %s', batchId, "start to train Model A-->B")
        train_ab(opt, output_ba, true)
        
        _G.logger:info('BATCH %d: %s', batchId, "start to train Model B-->A")
        train_ba(opt, output_ba, false)
      else
        -- sample sentences from language B
        _G.logger:info('BATCH %d: %s', batchId, "start to decode B-->A N-best")
        local output_ba = decode_n_best_ba(opt, srcBatch, batchId, nil, true)
        
        _G.logger:info('BATCH %d: %s', batchId, "start to decode A-->B 1-best")
        srcBatch = {} -- build srcBatch from previous output
        local output_ab = decode_n_best_ab(opt, srcBatch, batchId, output_ba, false)
        
        _G.logger:info('BATCH %d: %s', batchId, "start to train Model B-->A")
        train_ba(opt, output_ab, true)
        
        _G.logger:info('BATCH %d: %s', batchId, "start to train Model A-->B")
        train_ab(opt, output_ab, false)
      end
      
      if srcSeq == nil then
        break
      end
      
      batchId = batchId + 1
      srcBatch = {}
      output_ab = {}
      output_ba = {}
      
      flag_train_start_from_A = not flag_train_start_from_A
      collectgarbage()
      
      if batchId%opt.report_ppl_every_N_batches == 0 then
        _G.logger:info('%s', "start to evaluate ...")
        eval_and_save_ab(opt, false)
        eval_and_save_ba(opt, false)
      end
    end
  end

  _G.logger:info('%s', "start to evaluate and save epochs")
  eval_and_save_ab(opt, true)
  eval_and_save_ba(opt, true)
  
  _G.logger:shutDown()
end

main()
